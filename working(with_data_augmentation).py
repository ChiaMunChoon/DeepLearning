# -*- coding: utf-8 -*-
"""working(with Data Augmentation).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xww4AKWsSQNMkowTGrTpjkQsofv-45IO
"""

pip install scikeras

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from PIL import Image
from matplotlib.image import imread
import os
import glob as gb
import cv2
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras import layers
from keras import models
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.optimizers import Adam
from keras.callbacks  import EarlyStopping
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import kerastuner
from kerastuner.tuners import RandomSearch
from mlxtend.plotting import plot_confusion_matrix

tf.random.set_seed(42)

# Loading path directory
train_data = "../content/drive/MyDrive/Masters of Data Science/Sem 9 - Deep Learning/Assessment 3/melanoma_cancer_dataset/train/"
test_data = "../content/drive/MyDrive/Masters of Data Science/Sem 9 - Deep Learning/Assessment 3/melanoma_cancer_dataset/test/"

skin = os.listdir(train_data)
# Number of images for each disease
nums_train = {}
nums_val = {}
for s in skin:
    nums_train[s] = len(os.listdir(train_data + '/' + s))
img_per_class_train = pd.DataFrame(nums_train.values(), index=nums_train.keys(), columns=["no. of images"])
print('Train data distribution :')
img_per_class_train

# Data Distribution plot
plt.figure(figsize=(5,5))
plt.title('Data distribution ',fontsize=15)
plt.ylabel('Number of image',fontsize=10)
plt.xlabel('Type of skin cancer',fontsize=10)

keys = list(nums_train.keys())
vals = list(nums_train.values())
sns.barplot(x=keys, y=vals)

# Loading training images from folder to empty list
x_train = []
y_train = []

class_ = {'benign': 0, 'malignant': 1}

im_size = 150

for i in os.listdir(train_data):
    data_path = train_data + str(i)
    filenames = [i for i in os.listdir(data_path) ]
   ## Append label names accordingly
    for f in filenames:
        img = cv2.imread(data_path + '/' + f)
        img = img[:,:,::-1]
        img = cv2.resize(img, (im_size, im_size))
        x_train.append(img)
        y_train.append(class_[i])

# Loading testing images from folder to empty list
x_test = []
y_test = []

class_ = {'benign': 0, 'malignant': 1}

im_size = 150

for i in os.listdir(test_data):
    data_path = test_data + str(i)
    filenames = [i for i in os.listdir(data_path) ]
   ## Append label names accordingly
    for f in filenames:
        img = cv2.imread(data_path + '/' + f)
        img = img[:,:,::-1]
        img = cv2.resize(img, (im_size, im_size))
        x_test.append(img)
        y_test.append(class_[i])

# Convert data into NumPy Array
x_train = np.array(x_train)
y_train = np.array(y_train)
x_test = np.array(x_test)
y_test = np.array(y_test)

print(f'x_train shape  is {x_train.shape}')
print(f'y_train shape  is {y_train.shape}')

print(f'x_test shape  is {x_test.shape}')
print(f'y_test shape  is {y_test.shape}')

# Data Augmentation with ImageDataGenerator()

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    vertical_flip=True)

test_datagen = ImageDataGenerator(rescale = 1./255)

# Apply data augmentation to training and test dataset
train_datagen.fit(x_train)
test_datagen.fit(x_test)

print(f'x_train shape  is {x_train.shape}')
print(f'y_train shape  is {y_train.shape}')

print(f'x_test shape  is {x_test.shape}')
print(f'y_test shape  is {y_test.shape}')

# One-hot encoding
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Printing the shape
print(f'y_train shape  is {y_train.shape}')
print(f'y_test shape  is {y_test.shape}')

# Model Selection
# Initialize model sequential
model = Sequential()

# Input layer
model.add(Conv2D(32, 5, activation = 'relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(3))
model.add(Conv2D(64, 5, activation = 'relu'))
model.add(MaxPooling2D(3))
model.add(Conv2D(128, 5, activation = 'relu'))
model.add(MaxPooling2D(3))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
# Output layer
model.add(Dense(2, activation = 'softmax'))

# compile model
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])

# compile model
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience = 10)

# Model evaluation
history = model.fit(x_train, y_train, validation_split=0.2, epochs= 50, batch_size= 32, callbacks= [early_stopping])

# Model Prediction
testing_loss, testing_accuracy = model.evaluate(x_test, y_test)

# Plotting loss and accuracy evolution
plt.subplots(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Val_Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss evolution')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Val_accuracy')
plt.title(" Accuracy Evolution")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Model Prediction with Classification Report
preds = model.predict(x_test)

print(classification_report(np.argmax(y_test,axis=1), np.argmax(preds,axis=1)))

# calculating and plotting the confusion matrix
cm1 = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(preds,axis=1))
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

# Model Selection (Hyperparameter Tuning)
# Function to create model + adding parameter to tune
def create_model(hp):
  model = Sequential()
  hp_filters = hp.Int('filters', min_value = 32, max_value = 256, step = 32)
  hp_dropout = hp.Choice('dropout', values = [0.1,0.2,0.3])
  model.add(Conv2D(filters=hp_filters, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)))
  model.add(MaxPooling2D(3))
  model.add(Conv2D(64, (3, 3), activation = 'relu'))
  model.add(MaxPooling2D(3))
  model.add(Conv2D(128, (3, 3), activation = 'relu'))
  model.add(MaxPooling2D(3))
  model.add(Dropout(hp_dropout))
  model.add(Flatten())
  model.add(Dense(128, activation = 'relu'))
  model.add(Dropout(hp_dropout))
  # Output layer
  model.add(Dense(2, activation = 'softmax'))

  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])
  opt = Adam(learning_rate=hp_learning_rate)
  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
  return model

# Defining RandomSearch() tuner function
tuner = RandomSearch(
    create_model,
    objective='val_accuracy',
    executions_per_trial=1,
    directory='my_dir1')

tuner.search_space_summary()

stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

tuner.search(x_train, y_train, epochs=50, batch_size = 32, validation_split = 0.2, callbacks=[stop_early])

tuner.results_summary()

best_hyperparameters = tuner.get_best_hyperparameters(1)[0]
print(best_hyperparameters.values)

n_best_models = tuner.get_best_models(num_models=2)
print(n_best_models[0].summary())

model = tuner.hypermodel.build(best_hyperparameters)

history = model.fit(x_train, y_train, epochs=50, batch_size = 32, validation_split = 0.2)

# Plot loss and accuracy evolution graph
plt.subplots(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Val_Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss evolution')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Val_accuracy')
plt.title(" Accuracy Evolution")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Finding the optimal training epochs
val_acc_per_epoch = history.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

history = model.fit(x_train, y_train, epochs=best_epoch, batch_size = 32, validation_split = 0.2)

# Model Prediction
testing_loss, testing_accuracy = model.evaluate(x_test, y_test)

# Model Prediction with Classification Report
preds = model.predict(x_test)

print(classification_report(np.argmax(y_test,axis=1), np.argmax(preds,axis=1)))

# Plotting loss and accuracy evolution graphs
plt.subplots(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Val_Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss evolution')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Val_accuracy')
plt.title(" Accuracy Evolution")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# calculating and plotting the confusion matrix
cm1 = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(preds,axis=1))
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

class_dict = {0:'benign',
              1:'malignant'}

file_path =  '/benign/melanoma_10087.jpg'
test_image = cv2.imread(test_data + file_path)
test_image = test_image[:,:,::-1]
test_image = cv2.resize(test_image, (im_size, im_size))
plt.imshow(test_image)
test_image = np.expand_dims(test_image,axis=0)
probs = model.predict(test_image)
pred_class = np.argmax(probs)

pred_class = class_dict[pred_class]

print('prediction: ',pred_class)